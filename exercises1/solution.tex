\documentclass{scrartcl}

\input{setup}
\input{mathsetup}

\usepackage{enumitem}

\title{Machine Learning SS 2014}
\subtitle{Exercise Sheet 1}

\author{Georg Nebehay\\gnebehay@gmail.com
\and
Michael Meidlinger\\
michael.meidlinger@nt.tuwien.ac.at
}

\date{}

\tikzset{%
level 1/.style={sibling distance=10em},%
level 2/.style={sibling distance=7em},%
level 3/.style={sibling distance=3em}%
}

\begin{document}

\maketitle

\section{Decision Trees}

\input{trees}

\subsection{Additional Attributes}
There are $K=5$ attributes given, none of which a single split along any of those would achieve zero error probability. In order to achieve zero error probability with a single split along any of the $D$ additional attributes, at least one set of values of those additional attributes must be isomorphic to the dating decision. Let $\vec d \in \lbrace 0,1\rbrace^n$ denote the dating decisions of the $n$ candidates and $\rvec a_i \sim \frac{1}{2^n}$, $i=1,\dots,D$  random vectors representing the additional attributes. We can then derive the probability of zero error with a single split (event $\mathcal A$) as follows:

\begin{align}
	\Prob{\mathcal A}& = 
	\Prob{ \Big(\rvec a_1 = \vec d \vee \rvec a_1 = \neg \vec d \Big) \vee 
		\dots \vee \Big(\rvec a_D = \vec d \vee \rvec a_D = \neg \vec d \Big)} \\
	&= \sum_{i=1}^{D}  \left( \frac{2}{2^n} \right)^i  \left( 1-\frac{2}{2^n}\right)^{D-i} 
	= 1- \left( 1-2^{n-1} \right)^{D} 
	= \begin{cases} 0.0384 &D=10 \\  0.3293 &D=100 \\ 0.9950 &D=1353\end{cases}
\end{align}

\section{Nearest Neighbour Classification}

In this section we assume that the data is consistent,
meaning that for each pair of examples $(x_1,y_1), (x_2,y_2)$ the statement
$x_1 = x_2 \rightarrow y_1 = y_2$ is true.

\subsection{a)}

\mytodo{}

\subsection{b)}

Advantages of k-NN with $k > 1$ over NN are the following:
\begin{itemize}
  \item Robustness to outliers
  \item Model complexity can be steered
\end{itemize}

Disadvantages are:
\begin{itemize}
  \item Danger of underfitting
  \item Additional parameter that needs to be tuned
  \item More difficult to implement
\end{itemize}

\subsection{c)}
The training error of NN is always zero, as NN has capacity $\infty$ and completely memorizes the training set.
For any $x \in D$, the distance to itself is $\lVert x - x\rVert = 0$, meaning that all training examples will be
classified correctly.
For K-NN, this is no longer true, as for instance in a 3-NN classifier the next two neighbors might have a different label, thus misclassifying the training example.

\subsection{(d)}
Suppose we have calculated all the $d_i = \lVert x - x_i \rVert$ for some new sample $x$
and ordered the distances in increasing order.
For 2K-NN, we have a majority vote on the first 2k elements of this list.
There are 2 possibilites: Either there is one label that occurs more often than all the others.
Then the predicted label is identical to the output of a (2K-1)-NN classifier,
as there is always a margin of at least 2 since we have 2K elements in our list.
The other possibility is that there are two labels with the same number of occurrences.
In this case, the 2K-NN classifier predicts the majority label of the first (2K-1) elements.
As the first 2K-1 elements are now guaranteed to have a majority, this output is identical to a (2K-1)-NN
classifier. 

\section{Capacity \& Overfitting}
\subsection{Capacity}
We assume consistency of our training data: 
\begin{equation}
	(x^1,y^1),(x^2,y^2)\in \mathcal D: x^1=x^2 \Rightarrow y^1 =y^2.
\end{equation}
\begin{description}
	\item[Decision trees] If we use binary splits $\llbracket x_k \geq \theta_k\rrbracket$, $k=1,\dots,d$ we partition $\mathbb R^2$ into rectangular areas around the training data. If the data is consistent, we can make this partition arbitrarily granular and hence the memory is $\infty$.
	\item[1-NN] is always error free on the training data (cf. problem \ref{prob2}), thus we again have memory $\infty$.
	\item[$K$-NN] has capacity 2, since there is always the possibility that one sample with label $y^1$ is surrounded by $n-1$ samples with label $y^2$.
	\item[Perceptron] The decision $h(x) = \sign(\inprod{w}{x} -\theta)$ corresponds to a hyperplane decision boundary, where the hyperplane is determined by the weight vector $w$ and the offset $\theta$. Thus, to be error free on the training data, it has to be linearly separable, which is guaranteed for $n=2$ or less samples but not for 3 or more arbitrary samples (Imagine 3 samples that can be connected with a line:
	\begin{center}
		\begin{tikzpicture}
			\draw[fill=black] (0,0) circle [radius=2pt] node[below]{$y^1=-1$};
			\draw[fill=black]  (2,0) circle [radius=2pt] node[below]{$y^2=1$};
			\draw[fill=black]  (4,0) circle [radius=2pt] node[below]{$y^3=-1$};
			\draw (-1,0)--(5,0);
					\end{tikzpicture}
	\end{center}
	\item[Boosting] For this case, the memory depends on the simple component decision rules. In case of the binary coordinate splits that were used in the lecture slides, the situation is similar to the one of the decision tree. Therefore, the memory is $\infty$.
\end{description}
	
	\subsection{Overfitting}
	A high capacity obviously implies overfitting while a low one corresponds to unterfitting. 
	\subsection{Free parameters of the preceptron}
	In $\mathbb R^2$, there are $2$ weight vector elements and the offset $\theta$, thus in total $3$ parameters.
	\subsection{Free parameters of a tree with $L$ leaves}
	Each tree node checks $\llbracket x_k \geq \theta_k\rrbracket$, $k=1,\dots,d$ and hence contributes one parameter $\theta_i$. Consequently, the question reduces to finding the number $I$ of interior nodes of a full, binary tree with $L$ leaves. It can be shown that $I=L-1$ and therefore $L-1$ free parameters.
	
\section{Missing Proofs}
\subsection{Bayes Classifier}
The optimal classifier is given by
\begin{equation}
	c^\star(x) = \argmax_{y \in \mathcal Y} p(y| x) = \argmax_{y \in \mathcal Y} p(x,y),
\end{equation}
which for the binary case $y\in\lbrace -1,1\rbrace$ simplifies to
\begin{equation}
	c^\star(x) = \begin{cases} +1 &  p(+1|x) > p(-1|x) \\ -1 &\text{otherwise}\end{cases} 
	=  \sign \left( \log  \frac{p(+1|x)}{p(-1|x)}\right)
	=  \sign \left( \log  \frac{p(x,+1)}{p(x,-1)}\right),
\end{equation}
since $p(x,y)=p(y|x)p(x)$. \hfill $\blacksquare$

In communications, the quantities $\log  \frac{p(+1|x)}{p(-1|x)}$ are referred to as \emph{Log-Likelihood-Ratios (LLRs)} and are used in iterative decoding algorithms to quantify wether a bit was $+1$ or $-1$, given the received signal $x$.

\subsection{Minimum Risk Classifier}
The optimal classifier is given by
\begin{equation}
	c_\ell^\star(x) = \argmin_{y \in \mathcal Y}\E[\overline y \sim p(\overline y | x)][]{\ell(\overline y, y)}
					=  \argmin_{y \in \mathcal Y} \sum_{\overline y \in \mathcal Y} p(\overline y | x) \ell(\overline y, y) ,
\end{equation}
which for the binary case $y\in\lbrace -1,1\rbrace$ simplifies to
\begin{equation}
	c_\ell^\star(x) = \begin{cases} +1 &  p(+1|x)\underbrace{\ell(+1,+1)}_{d} + p(-1|x)\underbrace{\ell(-1,+1)}_{b} <
															p(+1|x)\underbrace{\ell(+1,-1)}_{c} + p(-1|x)\underbrace{\ell(-1,-1)}_{a} \\ 
													-1 &\text{otherwise}\end{cases} 
\end{equation}
and hence
\begin{equation}
	c_\ell^\star(x) =  \sign \left( \log  \frac{p(+1|x)(c-d)}{p(-1|x)(b-a)}\right) 
							= \sign \left( \log  \frac{p(+1|x)}{p(-1|x)} + \log{\frac{c-d}{b-a}}\right). \hfill \blacksquare
\end{equation}
\section{Practical Experiments I}

\subsection{Perceptron}

In the second dataset, the training algorithm does not converge as the examples
are no longer linearly separable.

\begin{tikzpicture}
  \begin{axis}[
    xlabel=Complexity,
    ylabel=Error,
  title=normal run]
    \addplot table[x expr=\coordindex+1,y index=0, header=false] {artificial1-trainerror-perc.txt};%
    \addplot table[x expr=\coordindex+1,y index=0, header=false] {artificial1-testerror-perc.txt};%
    \addlegendentry{training}%
    \addlegendentry{testing}%
  \end{axis}
  
\end{tikzpicture}
\begin{tikzpicture}
  \begin{axis}[
    xlabel=Complexity,
    ylabel=Error,
	mark repeat = 10,
  title=switched label of $y^9$]
	\addplot table[x expr=\coordindex+1,y index=0, header=false] {artificial2-trainerror-perc.txt};%
	\addplot table[x expr=\coordindex+1,y index=0, header=false] {artificial2-testerror-perc.txt};%
    \addlegendentry{training}%
    \addlegendentry{testing}%
  \end{axis}
  
\end{tikzpicture}

\subsection{AdaBoost}

\begin{tikzpicture}
  \begin{axis}[
    xlabel=Complexity,
    ylabel=Error,
  title=normal run]
	\addplot table[x expr=\coordindex+1,y index=0, header=false] {artificial1-trainerror-boost.txt};%
	\addplot table[x expr=\coordindex+1,y index=0, header=false] {artificial1-testerror-boost.txt};%
    \addlegendentry{training}%
    \addlegendentry{testing}%
  \end{axis}
  
\end{tikzpicture}
\begin{tikzpicture}
  \begin{axis}[
    xlabel=Complexity,
    ylabel=Error,
  title=switched label of $y^9$]
	\addplot table[x expr=\coordindex+1,y index=0, header=false] {artificial2-trainerror-boost.txt};%
	\addplot table[x expr=\coordindex+1,y index=0, header=false] {artificial2-testerror-boost.txt};%
    \addlegendentry{training}%
    \addlegendentry{testing}%
  \end{axis}
  
\end{tikzpicture}

\section{Practical Experiments II}
\subsection{AdaBoost}
In order to perform multiclass classification on the wine dataset using AdaBoost,
we employed a one-versus-all strategy.
We trained one classifier for each class,
using examples from one class as positive examples and
examples from all other classes as negative examples.
We combined the classifiers by predicting the label $y$ for which the corresponding classifier
reports the highest confidence score
%
\begin{equation}
  \hat y = \argmax_{k \in 1 \ldots K} f_k(x)  
\end{equation}
%
Using this method we achieve a test error of $E=8$ on the wine dataset.


\subsection{Decision Tree}
\begin{tikzpicture}
  \begin{axis}[
    xlabel=Complexity (\# of nodes),
    ylabel=Error]
	\addplot table[ header=false] {tree_continous/6a_train.txt};%
	\addplot table[header=false] {tree_continous/6a_test.txt};%};%
    \addlegendentry{training}%
    \addlegendentry{testing}%
  \end{axis}
\end{tikzpicture}
\clearpage

\section{Appendix}

\input{nosoccer}
\input{noeyes}
\input{itchyno}
\input{ralph}
  
\end{document}

